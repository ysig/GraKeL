{
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "version": "3.5.9",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "nbconvert_exporter": "python",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "name": "python"
    }
  },
  "nbformat": 4,
  "cells": [
    {
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "\n==============================================================================\nRetrieval of most similar document using the Weisfeiler-Lehman subtree kernel.\n==============================================================================\nScript makes use of :class:`grakel.WeisfeilerLehman`, :class:`grakel.VertexHistogram`\n\n"
      ]
    },
    {
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\nprint(__doc__)\n\nimport numpy as np\nimport time\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import sentence_polarity\n\nfrom grakel.kernels import WeisfeilerLehman, VertexHistogram\nfrom grakel import Graph\n\nsents = sentence_polarity.sents()\nsents = [sent for sent in sents if len(sent) > 1]\nn_sents = 3000\nsents = sents[:n_sents]\nprint(\"Loaded %d sentences\\n\" % n_sents)\n\nprint(\"Creating word co-occurrence networks\\n\")\nword_networks = list()\nfor sent in sents:\n\n    node_labels = dict()\n    tokens_to_ids = dict()\n    for token in sent:\n        if token not in tokens_to_ids:\n            tokens_to_ids[token] = len(tokens_to_ids)\n            node_labels[tokens_to_ids[token]] = token\n     \n    edges = list()\n    for i in range(len(sent)-1):\n        edges.append((tokens_to_ids[sent[i]], tokens_to_ids[sent[i+1]]))\n\n    word_networks.append(Graph(edges, node_labels=node_labels))\n\nquery_sent_id = 54\nquery_sent = [word_networks[query_sent_id]]\n\n# Initialize Weisfeiler-Lehman subtree kernel\ngk = WeisfeilerLehman(niter=2, normalize=True, base_kernel=VertexHistogram)\n\nprint(\"Computing similarities\\n\")\nt0 = time.time()\ngk.fit(query_sent)\nK = gk.transform(word_networks)\nprint(\"done in %0.3fs\\n\" % (time.time() - t0))\n\nprint(\"Query sentence\")\nprint(\"--------------\")\nprint(\" \".join(sents[query_sent_id]))\nprint()\nprint(\"Most similar sentence\")\nprint(\"---------------------\")\nprint(\" \".join(sents[np.argsort(K[:,0])[-2]]))"
      ],
      "outputs": []
    }
  ]
}