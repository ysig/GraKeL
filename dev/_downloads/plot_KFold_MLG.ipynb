{
  "nbformat_minor": 0,
  "nbformat": 4,
  "metadata": {
    "language_info": {
      "name": "python",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.5.5",
      "pygments_lexer": "ipython3",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      }
    },
    {
      "source": [
        "\n=========================================================================================\nKFold classification of a dataset [Cuneiform] using the fast Multiscale Laplacian kernel.\n=========================================================================================\n\nAn example plot of :class:`grakel.GraphKernel`, :class:`grakel.multiscale_laplacian_fast`\n\n"
      ],
      "metadata": {},
      "cell_type": "markdown"
    },
    {
      "outputs": [],
      "source": [
        "print(__doc__)\n\nimport argparse\n\nfrom grakel import GraphKernel\nfrom grakel import datasets\n\n# Create an argument parser for the installer of pynauty\nparser = argparse.ArgumentParser(\n    description='Measuring classification accuracy '\n                ' on multiscale_laplacian_fast')\n\nparser.add_argument(\n    '--dataset',\n    help='chose the datset you want the tests to be executed',\n    type=str,\n    default=\"Cuneiform\",\n)\n\n# Get the dataset name\ndataset_name = parser.parse_args().dataset\n\n# Check the dataset provided by the user\ndinfo = datasets.get_dataset_info(dataset_name)\nif dinfo is None:\n    raise TypeError('Dataset not found!')\nelif not dinfo[\"nl\"]:\n    raise TypeError('Dataset must have contain node attributes.')\n\n\n# The baseline dataset for node/edge-attributes\ndataset_attr = datasets.fetch_dataset(dataset_name,\n                                      with_classes=True,\n                                      prefer_attr_nodes=True,\n                                      prefer_attr_edges=True,\n                                      verbose=True)\n\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom time import time\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn import svm\n\ndef sec_to_time(sec):\n    \"\"\"Print time in a correct format.\"\"\"\n    dt = list()\n    days = int(sec // 86400)\n    if days > 0:\n        sec -= 86400*days\n        dt.append(str(days) + \" d\")\n\n    hrs = int(sec // 3600)\n    if hrs > 0:\n        sec -= 3600*hrs\n        dt.append(str(hrs) + \" h\")\n\n    mins = int(sec // 60)\n    if mins > 0:\n        sec -= 60*mins\n        dt.append(str(mins) + \" m\")\n\n    if sec > 0:\n        dt.append(str(round(sec, 2)) + \" s\")\n    return \" \".join(dt)\n\n# Loads the Mutag dataset from:\n# https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\n# the biggest collection of benchmark datasets for graph_kernels.\nG, y = dataset_attr.data, dataset_attr.target\nC_grid = (10. ** np.arange(4, 10, 1) / len(G)).tolist()\n\nstats = {\"acc\": list(), \"time\": list()}\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\nniter = kf.get_n_splits(y)\n\nfor train_index, test_index in tqdm(kf.split(G, y),\n                                    total=niter):\n    # Train-test split of graph data\n    tri = train_index.tolist()\n    tei = test_index.tolist()\n\n    G_train, G_test = list(), list()\n    y_train, y_test = list(), list()\n    for (i, (g, t)) in enumerate(zip(G, y)):\n        if len(tri) and i == tri[0]:\n            G_train.append(g)\n            y_train.append(t)\n            tri.pop(0)\n        elif len(tei) and i == tei[0]:\n            G_test.append(g)\n            y_test.append(t)\n            tei.pop(0)\n\n    start = time()\n    gk = GraphKernel(kernel={\"name\": \"multiscale_laplacian\", \"which\": \"fast\"})\n\n    # Calculate the kernel matrix.\n    K_train = gk.fit_transform(G_train)\n    K_test = gk.transform(G_test)\n    end = time()\n\n    # Cross validation on C, variable\n    acc = 0\n    for c in C_grid:\n        # Initialise an SVM and fit.\n        clf = svm.SVC(kernel='precomputed', C=c)\n\n        # Fit on the train Kernel\n        clf.fit(K_train, y_train)\n\n        # Predict and test.\n        y_pred = clf.predict(K_test)\n\n        # Calculate accuracy of classification.\n        acc = max(acc, accuracy_score(y_test, y_pred))\n\n    stats[\"acc\"].append(acc)\n    stats[\"time\"].append(end-start)\n\nprint(\"Mean values of \", niter, \"folds:\")\nprint(\"MLG [Fast] > Accuracy:\",\n      str(round(np.mean(stats[\"acc\"])*100, 2)),\n      \"% | Took:\", sec_to_time(np.mean(stats[\"time\"])))"
      ],
      "execution_count": null,
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      }
    }
  ]
}